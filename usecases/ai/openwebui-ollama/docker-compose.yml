# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  tts_service:
    build:
      context: ../microservices/text-to-speech
      dockerfile: Dockerfile
    image: tts_service
    hostname: tts_service
    container_name: tts_service
    privileged: true
    restart: always
    networks:
      - app-network
    environment:
      - TTS_DEVICE=CPU
    devices:
      - /dev:/dev:rw
      - /lib/modules:/lib/modules:rw
    command: "python3 -m uvicorn main:app --host tts_service --port 5995"

  stt_service:
    build:
      context: ../microservices/speech-to-text
      dockerfile: Dockerfile
    image: stt_service
    hostname: stt_service
    container_name: stt_service
    privileged: true
    group_add:
      - ${RENDER_GROUP_ID:-110}
    networks:
      - app-network
    ports:
      - 5996:5996
    environment:
      - DEFAULT_MODEL_ID=openai/whisper-tiny 
      - STT_DEVICE=CPU
      - ALLOWED_CORS=["http://localhost"]
    restart: always
    volumes:
      - ./data:/usr/src/app/data:rw
    devices:
      - /dev:/dev
      - /lib/modules:/lib/modules
    command: "python3 -m uvicorn server:app --host stt_service --port 5996"

  open-webui:
    hostname: open-webui
    container_name: open-webui
    image: ghcr.io/open-webui/open-webui:main
    restart: always
    ports:
      - 80:8080
    depends_on:
      stt_service:
        condition: service_healthy
      tts_service:
        condition: service_healthy
    networks:
      - app-network
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - AUDIO_STT_OPENAI_API_BASE_URL=http://stt_service:5996/v1
      - AUDIO_STT_OPENAI_API_KEY=-
    volumes:
      - open-webui:/app/backend/data

  ollama:
    build:
      context: ../microservices/ollama
      dockerfile: Dockerfile
    hostname: ollama
    container_name: ollama
    image: intel-ipex-ollama
    restart: always
    privileged: true
    networks:
      - app-network
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1
      - SYCL_CACHE_PERSISTENT=1
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1
      - ONEAPI_DEVICE_SELECTOR=level_zero:0
    devices:
      - /dev:/dev:rw
      - /lib/modules:/lib/modules:rw
    volumes:
      - ./serving/ollama/setup.sh:/setup.sh
      - ollama:/root/.ollama
    command: "./ollama serve"

networks:
  app-network:

volumes:
  ollama:
  open-webui: