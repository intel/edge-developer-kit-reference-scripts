# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  ui:
    build:
      context: ./edge-ui
      dockerfile: Dockerfile
    image: edge-ai-tuning-kit.deployment.ui
    container_name: edge-ai-tuning-kit.deployment.ui
    hostname: ui
    depends_on:
      backend:
        condition: service_healthy
    networks:
      - app-network
    ports:
      - "127.0.0.1:8010:3000"

  backend:
    build:
      context: .
      dockerfile: ./backend/Dockerfile
    image: edge-ai-tuning-kit.deployment.backend
    container_name: edge-ai-tuning-kit.deployment.backend
    hostname: backend
    group_add:
      - ${RENDER_GROUP_ID:-110}
    devices:
      - /dev/dri:/dev/dri
    networks:
      - app-network
    ports:
      - "127.0.0.1:8011:8011"
    environment:
      - OPENAI_BASE_URL=http://serving:8000/v1
      - ALLOWED_CORS=["http://localhost:8010"]
      - EMBEDDING_DEVICE=${EMBEDDING_DEVICE:-CPU}
      - RERANKER_DEVICE=${RERANKER_DEVICE:-CPU}
    depends_on:
      serving:
        condition: service_healthy
      tts_service:
        condition: service_healthy
      stt_service:
        condition: service_healthy
    volumes:
      - ./data:/usr/src/app/data

  serving:
    build:
      context: ../microservices/text-generation/vllm
      dockerfile: Dockerfile
    image: edge-ai-tuning-kit.deployment.serving
    container_name: edge-ai-tuning-kit.deployment.serving
    hostname: serving
    group_add:
      - ${RENDER_GROUP_ID:-110}
    devices:
      - /dev/dri:/dev/dri
    networks:
      - app-network
    ports:
      - "127.0.0.1:8012:8000"
    environment:
      - DEFAULT_MODEL_ID=Qwen/Qwen2.5-7B-Instruct
      - MODEL_PATH=./data/models/llm
      - MODEL_PRECISION=int4
      - SERVED_MODEL_NAME=ov-vllm
      - MAX_MODEL_LEN=2048
      - MAX_NUM_SEQS=1
      - GPU_MEMORY_UTILIZATION=0.9
      - VLLM_OPENVINO_DEVICE=${LLM_DEVICE:-GPU}
      - VLLM_OPENVINO_KVCACHE_SPACE=4
    volumes:
      - ./data:/usr/src/app/data

  tts_service:
    build:
      context: ../microservices/text-to-speech
      dockerfile: Dockerfile
    image: tts_service
    container_name: edge-ai-tuning-kit.deployment.tts
    hostname: tts_service
    group_add:
      - ${RENDER_GROUP_ID:-110}
    restart: always
    networks:
      - app-network
    environment:
      - TTS_DEVICE=${TTS_DEVICE:-CPU}
      - BERT_DEVICE=${BERT_DEVICE:-CPU}
      - ALLOWED_CORS=["*"]
    ports:
      - 127.0.0.1:8013:5995
    devices:
      - /dev/dri:/dev/dri
    volumes:
      - tts-data:/usr/src/app/data

  stt_service:
    build:
      context: ../microservices/speech-to-text
      dockerfile: Dockerfile
    image: stt_service
    container_name: edge-ai-tuning-kit.deployment.stt
    hostname: stt_service
    group_add:
      - ${RENDER_GROUP_ID:-110}
    networks:
      - app-network
    environment:
      - DEFAULT_MODEL_ID=openai/whisper-tiny 
      - STT_DEVICE=CPU
      - ALLOWED_CORS=["*"]
    ports:
      - 127.0.0.1:8014:5996
    devices:
      - /dev/dri:/dev/dri
    volumes:
      - stt-data:/usr/src/app/data:rw

networks:
  app-network:

volumes:
  tts-data:
  stt-data:
