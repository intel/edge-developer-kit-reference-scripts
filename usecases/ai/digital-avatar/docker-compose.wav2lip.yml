# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0


services:
  wav2lip:
    build:
      context: .
      dockerfile: ./backend/wav2lip/Dockerfile
      args:
        - RENDER_GROUP_ID=${RENDER_GROUP_ID:-992} 
    image: digital-avatar.backend.wav2lip
    container_name: digital-avatar.backend.wav2lip
    privileged: true
    restart: always
    # depends_on:
      # liveportrait:
      #   condition: service_healthy
    devices:
      - /dev:/dev:rw
    environment:
      - SERVER_PORT=8011
      - DEVICE=CPU
      - ALLOWED_CORS=["http://localhost"] # frontend url
    # group_add:
    #   - ${RENDER_GROUP_ID:-992}
    volumes:
      - ./weights/checkpoints:/usr/src/app/wav2lip/checkpoints:rw
      - ./assets:/usr/src/app/assets
      - ./weights/wav2lip_gan.pth:/usr/src/app/setup/wav2lip_gan.pth
      - ./data/audio:/usr/src/app/data
    ports:
      - 8011:8011
    command: "python3 main.py"

  liveportrait:
    build:
      context: .
      dockerfile: ./backend/liveportrait/Dockerfile
      args:
        - RENDER_GROUP_ID=${RENDER_GROUP_ID:-992} 
    image: digital-avatar.backend.liveportrait
    container_name: digital-avatar.backend.liveportrait
    privileged: true
    restart: always
    group_add:
      - ${RENDER_GROUP_ID:-992}
    environment:
      - SERVER_PORT=8012
      - ALLOWED_CORS=["http://localhost"] # frontend url
      - ONEAPI_DEVICE_SELECTOR=level_zero:0
    devices:
      - /dev:/dev:rw
    volumes:
      - ./assets:/usr/src/app/assets
      - ./data/audio:/usr/src/app/data
    ports:
      - 8012:8012
    command: "python3 main.py"

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
      args:
        - NEXT_PUBLIC_LLM_MODEL=${LLM_MODEL:-qwen2.5}
        - NEXT_PUBLIC_TTS_URL=localhost:8013
        - NEXT_PUBLIC_LLM_URL=ollama:11434
        - NEXT_PUBLIC_STT_URL=localhost:8014
        - NEXT_PUBLIC_LIPSYNC_URL=localhost:8011
    image: digital-avatar.frontend
    container_name: digital-avatar.frontend
    restart: always
    depends_on:
      liveportrait:
        condition: service_healthy
      wav2lip:
        condition: service_healthy
      ollama:
        condition: service_healthy
      stt_service:
        condition: service_healthy
      tts_service:
        condition: service_healthy
    networks:
      - app-network
    volumes:
      - ./assets:/app/public/assets
    ports:
      - "80:3000"
    command: "npm start"

  tts_service:
    build:
      context: ../microservices/piper-tts
      dockerfile: Dockerfile
    image: tts_service
    hostname: tts_service
    container_name: tts_service
    restart: always
    networks:
      - app-network
    environment:
      - ALLOWED_CORS=["http://localhost"] # frontend url
      - SERVER_PORT=8013
    ports:
      - 8013:8013
    volumes:
      - ./data/piper:/models:rw
      - ./data/audio:/data:rw
    command: python3 main.py

  stt_service:
    build:
      context: ../microservices/speech-to-text
      dockerfile: Dockerfile
    image: stt_service
    container_name: stt_service
    hostname: stt_service
    group_add:
      - ${RENDER_GROUP_ID:-992}
    networks:
      - app-network
    environment:
      - DEFAULT_MODEL_ID=openai/whisper-tiny 
      - STT_DEVICE=CPU
      - ALLOWED_CORS=["*"]
    ports:
      - 8014:5996
    devices:
      - /dev/dri:/dev/dri
    volumes:
      - ./data:/usr/src/app/data:rw

  ollama:
    build:
      context: ../microservices/ollama
      dockerfile: Dockerfile
    hostname: ollama
    container_name: ollama
    image: intel-ipex-ollama
    restart: always
    privileged: true
    ports:
      - 8015:8015
    environment:
      - OLLAMA_HOST=0.0.0.0:8015
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1
      - SYCL_CACHE_PERSISTENT=1
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1
      - ONEAPI_DEVICE_SELECTOR=level_zero:0
      - LLM_MODEL=${LLM_MODEL:-qwen2.5}
      - OLLAMA_KEEP_ALIVE=-1
    networks:
      - app-network
    devices:
      - /dev:/dev:rw
      - /lib/modules:/lib/modules:rw
    volumes:
      - ./data/ollama:/home/ubuntu/.ollama
      - ./scripts/run.sh:/opt/intel/llm-app/run.sh
      - ./scripts/healthcheck.sh:/opt/intel/llm-app/healthcheck.sh
    healthcheck:
      test: ["CMD", "bash", "/opt/intel/llm-app/healthcheck.sh"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s
    command: "./run.sh"

networks:
  app-network:
