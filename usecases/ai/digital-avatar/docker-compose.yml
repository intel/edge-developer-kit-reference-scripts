# Copyright (C) 2024 Intel Corporation
# SPDX-License-Identifier: Apache-2.0

services:
  # lipsync:
  #   build:
  #     context: .
  #     dockerfile: ./backend/sadtalker/Dockerfile
  #     args:
  #       - RENDER_GROUP_ID=${RENDER_GROUP_ID:-992} 
  #   image: digital-avatar.backend.sadtalker
  #   container_name: digital-avatar.backend.sadtalker
  #   privileged: true
  #   restart: always
  #   # depends_on:
  #     # liveportrait:
  #     #   condition: service_healthy
  #   devices:
  #     - /dev:/dev:rw
  #   environment:
  #     - SERVER_PORT=8011
  #     - DEVICE=xpu
  #     - ALLOWED_CORS=["http://localhost"] # frontend url
  #   group_add:
  #     - ${RENDER_GROUP_ID:-992}
  #   volumes:
  #     - ./assets:/usr/src/app/assets
  #     - ./data/sadtalker:/usr/src/app/results
  #     - ./data/audio:/usr/src/app/data
  #   networks:
  #     - app-network
  #   ports:
  #     - 8011:8011
  #   command: "python3 main.py"

  lipsync:
    build:
      context: .
      dockerfile: ./backend/wav2lip/Dockerfile
      args:
        - RENDER_GROUP_ID=${RENDER_GROUP_ID:-992} 
    image: digital-avatar.backend.lipsync
    container_name: digital-avatar.backend.lipsync
    privileged: true
    restart: always
    # depends_on:
      # liveportrait:
      #   condition: service_healthy
    devices:
      - /dev:/dev:rw
    networks:
      - app-network
    environment:
      - SERVER_PORT=8011
      - DEVICE=CPU
      - ALLOWED_CORS=["http://localhost"] # frontend url
    # group_add:
    #   - ${RENDER_GROUP_ID:-992}
    volumes:
      - ./weights/checkpoints:/usr/src/app/wav2lip/checkpoints:rw
      - ./assets:/usr/src/app/assets
      - ./weights/wav2lip_gan.pth:/usr/src/app/setup/wav2lip_gan.pth
      - ./data/audio:/usr/src/app/data
      - ./data/wav2lip:/usr/src/app/wav2lip/results
    ports:
      - 8011:8011
    command: "python3 main.py"

  frontend:
    build:
      context: .
      dockerfile: frontend/Dockerfile
      args:
        - NEXT_PUBLIC_LLM_MODEL=${LLM_MODEL:-qwen2.5}
        - NEXT_PUBLIC_TTS_URL=tts_service:8013
        - NEXT_PUBLIC_LLM_URL=rag-backend:8012
        - NEXT_PUBLIC_STT_URL=stt_service:5996
        - NEXT_PUBLIC_LIPSYNC_URL=lipsync:8011
    image: digital-avatar.frontend
    container_name: digital-avatar.frontend
    restart: always
    depends_on:
      lipsync:
        condition: service_healthy
      ollama:
        condition: service_healthy
      stt_service:
        condition: service_healthy
      tts_service:
        condition: service_healthy
    networks:
      - app-network
    volumes:
      - ./assets:/app/public/assets
    ports:
      - "80:3000"
    command: "npm start"

  tts_service:
    build:
      context: ../microservices/piper-tts
      dockerfile: Dockerfile
    image: tts_service
    hostname: tts_service
    container_name: tts_service
    restart: always
    networks:
      - app-network
    environment:
      - ALLOWED_CORS=["*"]
      - SERVER_PORT=8013
    ports:
      - 8013:8013
    volumes:
      - ./data/piper:/usr/src/app/models:rw
      - ./data/audio:/usr/src/app/data:rw
    command: python3 main.py
    
  stt_service:
    build:
      context: ../microservices/speech-to-text
      dockerfile: Dockerfile
    image: stt_service
    container_name: stt_service
    hostname: stt_service
    group_add:
      - ${RENDER_GROUP_ID:-992}
    networks:
      - app-network
    environment:
      - DEFAULT_MODEL_ID=openai/whisper-medium
      - STT_DEVICE=CPU
      - ALLOWED_CORS=["*"]
    ports:
      - 8014:5996
    devices:
      - /dev/dri:/dev/dri
    volumes:
      - ./data:/usr/src/app/data:rw

  ollama:
    build:
      context: ../microservices/ollama
      dockerfile: Dockerfile
    hostname: ollama
    container_name: ollama
    image: intel-ipex-ollama
    restart: always
    privileged: true
    ports:
      - 8015:11434
    environment:
      - OLLAMA_HOST=0.0.0.0:11434
      - OLLAMA_NUM_GPU=999
      - ZES_ENABLE_SYSMAN=1
      - SYCL_CACHE_PERSISTENT=1
      - SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS=1
      - ONEAPI_DEVICE_SELECTOR=level_zero:0
      - LLM_MODEL=${LLM_MODEL:-qwen2.5}
      - OLLAMA_KEEP_ALIVE=-1
    networks:
      - app-network
    devices:
      - /dev:/dev:rw
      - /lib/modules:/lib/modules:rw
    volumes:
      - ./data/ollama:/root/.ollama
      - ./scripts/run.sh:/opt/intel/llm-app/run.sh
      - ./scripts/healthcheck.sh:/opt/intel/llm-app/healthcheck.sh
    healthcheck:
      test: ["CMD", "bash", "/opt/intel/llm-app/healthcheck.sh"]
      interval: 1m30s
      timeout: 30s
      retries: 5
      start_period: 30s
    command: "./run.sh"

  rag-backend:
    build:
      context: ./backend/rag-backend
    image: digital-avatar.backend.rag
    container_name: digital-avatar.backend.rag
    hostname: rag-backend
    group_add:
      - ${RENDER_GROUP_ID:-992}
    devices:
      - /dev/dri:/dev/dri
    networks:
      - app-network
    ports:
      - "8012:8012"
    environment:
      - OPENAI_BASE_URL=http://ollama:11434/v1
      - OPENAI_API_KEY=ollama
      - EMBEDDING_DEVICE=${EMBEDDING_DEVICE:-CPU}
      - RERANKER_DEVICE=${RERANKER_DEVICE:-CPU}
      - SERVER_PORT=8012
    depends_on:
      ollama:
        condition: service_healthy
      tts_service:
        condition: service_healthy
      stt_service:
        condition: service_healthy
    volumes:
      - ./data:/usr/src/app/data

networks:
  app-network:
